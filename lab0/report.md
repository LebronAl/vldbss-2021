# Lab0 实验报告

## 实验结果

![](https://user-images.githubusercontent.com/32640567/127286350-e0d417ce-b79a-4dea-9e7d-3e45b5650872.png)

### 1. 完成 Map-Reduce 框架

实现思路参照了之前做 6.824 lab1 的[思路](https://github.com/LebronAl/MIT6.824-2021/blob/master/docs/lab1.md)，不过这里不需要考虑容错，所以更加简单。

注：在 map 的实现中，编码并持久化多个 reducer 的输入文件时可以并行，这是一个比较重要的优化点。  

**耗时如上图最左边所示，约 336s。**

### 2. 基于 Map-Reduce 框架编写 Map-Reduce 函数

实现了两个版本：基本版和优化版。

对于基本版，其相比示例版主要做了两点优化：
* 第一轮 count 时实现 mapper 端的 Combiner：对于聚合类型的需求，可以在 mapper 端做一层聚合，这是很基本的 MR 优化。
* 第二轮 topK 时实现 mapper 端的过滤：对于 topK 的需求，mapper 端可能会发送很多记录，但其实最多只会有 K 个有用，所以可以先在 mapper 端找到 topK，再发给 reducer 让其 nMap * K 个元素中进一步找 topK。

对于这两个优化，他们的主要目的都是减少传输和计算的数据量。这样一方面可以减少磁盘和网络 IO 耗时（mapper 端写文件，reducer 端拉文件等），另一方面也可以降低 CPU 压力（mapper 端编码，reducer 端解码，排序等），从而大幅度提升性能。**这两点优化效果很明显，性能提升了约 10 倍，耗时约 35s。**

对于优化版，其相对基本版在第二轮求 topK 的排序部分又做了优化，理论上示例版和基本版此部分的排序都是将所有元素进行了快排，理论时间复杂度是 O(nlgn)，然而求 TopK 实际上可以利用快排算法+随机选择来将复杂度降低到 O(n)，因此优化版进行了该尝试。**可以看到有一定效果，但不是很明显，耗时降低到了 34s。**

对于不同的数据分布，简单看了一下经过基本版的优化后耗时均已大幅度降低，仅有 case 4 耗时仍然较长，这主要是由于其产生了 100 万 url，这个数量级至少是其他 case 的 10 倍，甚至还可能是成百上千倍，所以前面提到的数据量减少优化并没有起到太多作用。要想优化这种场景，感觉 MR 已经帮助不了什么了，可以尝试从压缩，编码，语言特性等方面尝试去进一步优化。

## 实验总结

由于之前做过 6.824 的 lab，所以实验过程中没遇到什么困难。

对于 MR 计算框架，个人感觉这是分布式计算一个很本质的思想，不论是 Spark，Flink，Hive 还是 MPP 类数据库，其底层的思想都是通过 map 和 reduce 去将计算负载路由到所有节点上，只不过上层针对不同地场景和需求有了一些优化而已。

因此，MapReduce YYDS。